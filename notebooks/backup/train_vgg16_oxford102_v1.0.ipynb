{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow\n",
    "from time import time\n",
    "\n",
    "import pandas\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/oxford102/train'\n",
    "img_width, img_height = 256, 256\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "nr_categories = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"/data/oxford102/train/\"\n",
    "validation_data_dir = \"/data/oxford102/train/\"\n",
    "nb_train_samples = 4604\n",
    "nb_validation_samples = 1094 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processor = keras.applications.vgg16.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_datagen = ImageDataGenerator(preprocessing_function=input_processor, \n",
    "                                           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "train_generator = train_val_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        batch_size=batch_size,\n",
    "        subset=\"training\",\n",
    "        class_mode='categorical')\n",
    "validation_generator = train_val_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        subset=\"validation\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "k.set_learning_phase(0)\n",
    "\n",
    "network_name = \"vgg16\"\n",
    "img_width, img_height = (256, 256)\n",
    "if network_name == \"vgg16\":\n",
    "    base_model = keras.applications.vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "elif network_name == \"vgg19\":\n",
    "    base_model = keras.applications.vgg19.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "else:\n",
    "    raise Exception(\"check your network name\")\n",
    "\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    #Adding custom Layers \n",
    "k.set_learning_phase(1)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "#x = Dense(102, activation=\"relu\")(x)\n",
    "predictions = Dense(nr_categories, activation=\"softmax\")(x)\n",
    "\n",
    "_model = Model(input = base_model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 102)               104550    \n",
      "=================================================================\n",
      "Total params: 48,378,790\n",
      "Trainable params: 33,662,054\n",
      "Non-trainable params: 14,716,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 18s 505ms/step - loss: 4.5033 - acc: 0.1063 - val_loss: 3.5819 - val_acc: 0.2383\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.23828, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 17s 474ms/step - loss: 3.0453 - acc: 0.3232 - val_loss: 2.6389 - val_acc: 0.4062\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.23828 to 0.40625, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 17s 477ms/step - loss: 2.4431 - acc: 0.4427 - val_loss: 2.2150 - val_acc: 0.4727\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.40625 to 0.47266, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 17s 474ms/step - loss: 2.0987 - acc: 0.5152 - val_loss: 1.8770 - val_acc: 0.5898\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.47266 to 0.58984, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 17s 489ms/step - loss: 1.2158 - acc: 0.7652 - val_loss: 1.7634 - val_acc: 0.5977\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58984 to 0.59766, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 17s 474ms/step - loss: 1.0884 - acc: 0.8009 - val_loss: 1.6001 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59766 to 0.66406, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 17s 475ms/step - loss: 1.0199 - acc: 0.7977 - val_loss: 1.5470 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.66406\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 17s 473ms/step - loss: 0.9088 - acc: 0.8295 - val_loss: 1.4728 - val_acc: 0.6484\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.66406\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 17s 487ms/step - loss: 0.6411 - acc: 0.8991 - val_loss: 1.4696 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.66406 to 0.67969, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 17s 482ms/step - loss: 0.5429 - acc: 0.9366 - val_loss: 1.4128 - val_acc: 0.6914\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.67969 to 0.69141, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 17s 477ms/step - loss: 0.5317 - acc: 0.9310 - val_loss: 1.3570 - val_acc: 0.7109\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.69141 to 0.71094, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 17s 473ms/step - loss: 0.4850 - acc: 0.9393 - val_loss: 1.2813 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.71094\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 18s 502ms/step - loss: 0.3866 - acc: 0.9584 - val_loss: 1.2790 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.71094 to 0.71875, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 17s 475ms/step - loss: 0.3347 - acc: 0.9598 - val_loss: 1.2524 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.71875 to 0.72656, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 17s 474ms/step - loss: 0.3183 - acc: 0.9696 - val_loss: 1.1352 - val_acc: 0.7383\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.72656 to 0.73828, saving model to /data/oxford102/experiments/1542069202.4178612/vgg16_1542069202.4178612.h5\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 17s 473ms/step - loss: 0.3174 - acc: 0.9723 - val_loss: 1.1999 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.73828\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 17s 498ms/step - loss: 0.2624 - acc: 0.9750 - val_loss: 1.1064 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.73828\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 17s 480ms/step - loss: 0.2094 - acc: 0.9955 - val_loss: 1.2018 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.73828\n",
      "Epoch 00018: early stopping\n",
      "{'network_name': 'vgg16', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1542069202.4178612}\n"
     ]
    }
   ],
   "source": [
    "#run_training(model_name, _model, train_generator, validation_generator, params, num_train_img, num_val_img):\n",
    "    \n",
    "\n",
    "model_name = \"vgg16\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg16',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 128,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 35s 489ms/step - loss: 0.1904 - acc: 0.9911 - val_loss: 1.1201 - val_acc: 0.7261\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72610, saving model to /data/oxford102/experiments/1542069948.9042153/vgg16_1542069948.9042153.h5\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 33s 468ms/step - loss: 0.1699 - acc: 0.9921 - val_loss: 1.0966 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.72610 to 0.75000, saving model to /data/oxford102/experiments/1542069948.9042153/vgg16_1542069948.9042153.h5\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 35s 487ms/step - loss: 0.1297 - acc: 0.9965 - val_loss: 1.0767 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.75000\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 34s 473ms/step - loss: 0.1251 - acc: 0.9960 - val_loss: 1.0593 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.75000 to 0.77022, saving model to /data/oxford102/experiments/1542069948.9042153/vgg16_1542069948.9042153.h5\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 35s 493ms/step - loss: 0.1020 - acc: 0.9978 - val_loss: 1.0474 - val_acc: 0.7592\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77022\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 34s 474ms/step - loss: 0.1022 - acc: 0.9978 - val_loss: 1.0574 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77022\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 34s 483ms/step - loss: 0.0842 - acc: 0.9991 - val_loss: 1.0093 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77022\n",
      "Epoch 00007: early stopping\n",
      "{'network_name': 'vgg16', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 64, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1542069948.9042153}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg16\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "\n",
    "params = {'network_name': 'vgg16',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 64,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "143/143 [==============================] - 69s 485ms/step - loss: 0.0780 - acc: 0.9978 - val_loss: 1.0344 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76287, saving model to /data/oxford102/experiments/1542070195.6867728/vgg16_1542070195.6867728.h5\n",
      "Epoch 2/100\n",
      "143/143 [==============================] - 70s 488ms/step - loss: 0.0669 - acc: 0.9987 - val_loss: 1.0309 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76287\n",
      "Epoch 3/100\n",
      "143/143 [==============================] - 69s 483ms/step - loss: 0.0578 - acc: 0.9998 - val_loss: 1.0022 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76287 to 0.77390, saving model to /data/oxford102/experiments/1542070195.6867728/vgg16_1542070195.6867728.h5\n",
      "Epoch 4/100\n",
      "143/143 [==============================] - 69s 481ms/step - loss: 0.0512 - acc: 0.9998 - val_loss: 1.0075 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77390\n",
      "Epoch 5/100\n",
      "143/143 [==============================] - 69s 483ms/step - loss: 0.0441 - acc: 1.0000 - val_loss: 0.9698 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77390\n",
      "Epoch 6/100\n",
      "143/143 [==============================] - 69s 483ms/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.9722 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77390\n",
      "Epoch 00006: early stopping\n",
      "{'network_name': 'vgg16', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 32, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1542070195.6867728}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg16\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg16',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 32,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "287/287 [==============================] - 139s 483ms/step - loss: 0.0335 - acc: 0.9999 - val_loss: 1.0041 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75721, saving model to /data/oxford102/experiments/1542070615.581809/vgg16_1542070615.581809.h5\n",
      "Epoch 2/100\n",
      "287/287 [==============================] - 139s 484ms/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.9644 - val_acc: 0.7749\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75721 to 0.77488, saving model to /data/oxford102/experiments/1542070615.581809/vgg16_1542070615.581809.h5\n",
      "Epoch 3/100\n",
      "287/287 [==============================] - 139s 484ms/step - loss: 0.0255 - acc: 1.0000 - val_loss: 0.9489 - val_acc: 0.7674\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.77488\n",
      "Epoch 4/100\n",
      "287/287 [==============================] - 138s 482ms/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.9373 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77488 to 0.77953, saving model to /data/oxford102/experiments/1542070615.581809/vgg16_1542070615.581809.h5\n",
      "Epoch 5/100\n",
      "287/287 [==============================] - 137s 478ms/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.9373 - val_acc: 0.7767\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77953\n",
      "Epoch 6/100\n",
      "287/287 [==============================] - 138s 479ms/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.9066 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77953 to 0.78372, saving model to /data/oxford102/experiments/1542070615.581809/vgg16_1542070615.581809.h5\n",
      "Epoch 7/100\n",
      "287/287 [==============================] - 139s 483ms/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.8953 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78372\n",
      "Epoch 8/100\n",
      "287/287 [==============================] - 139s 485ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.9187 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.78372\n",
      "Epoch 9/100\n",
      "287/287 [==============================] - 138s 480ms/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.8915 - val_acc: 0.7935\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.78372 to 0.79349, saving model to /data/oxford102/experiments/1542070615.581809/vgg16_1542070615.581809.h5\n",
      "Epoch 10/100\n",
      "287/287 [==============================] - 138s 481ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.8832 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79349\n",
      "Epoch 11/100\n",
      "287/287 [==============================] - 138s 482ms/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.8670 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79349\n",
      "Epoch 12/100\n",
      "287/287 [==============================] - 139s 483ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.8664 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79349\n",
      "Epoch 00012: early stopping\n",
      "{'network_name': 'vgg16', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 16, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1542070615.581809}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg16\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg16',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 16,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNE TOP LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 block4_conv1\n",
      "1 block4_conv2\n",
      "2 block4_conv3\n",
      "3 block4_pool\n",
      "4 block5_conv1\n",
      "5 block5_conv2\n",
      "6 block5_conv3\n",
      "7 block5_pool\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers[11:]):\n",
    "    print(i, layer.name)\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "287/287 [==============================] - 181s 632ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.8273 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80000, saving model to /data/oxford102/experiments/1542073918.6993766/vgg16_1542073918.6993766.h5\n",
      "Epoch 2/100\n",
      "287/287 [==============================] - 178s 621ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.8125 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.80000\n",
      "Epoch 3/100\n",
      "287/287 [==============================] - 178s 621ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.7900 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.80000 to 0.81302, saving model to /data/oxford102/experiments/1542073918.6993766/vgg16_1542073918.6993766.h5\n",
      "Epoch 4/100\n",
      "287/287 [==============================] - 178s 621ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.7772 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.81302\n",
      "Epoch 5/100\n",
      "287/287 [==============================] - 178s 622ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.7853 - val_acc: 0.8023\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81302\n",
      "Epoch 6/100\n",
      "287/287 [==============================] - 178s 621ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.7856 - val_acc: 0.8121\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81302\n",
      "Epoch 00006: early stopping\n",
      "{'network_name': 'vgg16', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 16, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1542073918.6993766}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg16\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg16',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 16,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
