{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow\n",
    "from time import time\n",
    "\n",
    "import pandas\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/oxford102/train'\n",
    "img_width, img_height = 256, 256\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "nr_categories = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"/data/oxford102/train/\"\n",
    "validation_data_dir = \"/data/oxford102/train/\"\n",
    "nb_train_samples = 4604\n",
    "nb_validation_samples = 1094 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processor = keras.applications.vgg19.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_datagen = ImageDataGenerator(preprocessing_function=input_processor, \n",
    "                                           validation_split=0.2)\n",
    "\n",
    "train_val_datagen_aug = ImageDataGenerator(\n",
    "        #rescale=1. / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=input_processor,\n",
    "        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "train_generator = train_val_datagen_aug.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        batch_size=batch_size,\n",
    "        subset=\"training\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_val_datagen_aug.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        subset=\"validation\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "k.set_learning_phase(0)\n",
    "\n",
    "network_name = \"vgg19\"\n",
    "img_width, img_height = (256, 256)\n",
    "if network_name == \"vgg16\":\n",
    "    base_model = keras.applications.vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "elif network_name == \"vgg19\":\n",
    "    base_model = keras.applications.vgg19.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "else:\n",
    "    raise Exception(\"check your network name\")\n",
    "\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    #Adding custom Layers \n",
    "k.set_learning_phase(1)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\", \n",
    "          kernel_regularizer=regularizers.l2(0.01),\n",
    "         #       activity_regularizer=regularizers.l1(0.001)\n",
    "         )(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "#x = Dense(102, activation=\"relu\")(x)\n",
    "predictions = Dense(nr_categories, activation=\"softmax\")(x)\n",
    "\n",
    "_model = Model(input = base_model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 102)               104550    \n",
      "=================================================================\n",
      "Total params: 53,688,486\n",
      "Trainable params: 33,662,054\n",
      "Non-trainable params: 20,026,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 25.2712 - acc: 0.0078 - val_loss: 25.0739 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00000, saving model to /data/oxford102/experiments/1543117119.2325218/vgg19_1543117119.2325218.h5\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 6s 704ms/step - loss: 25.0496 - acc: 0.0234 - val_loss: 24.7788 - val_acc: 0.0469\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.00000 to 0.04688, saving model to /data/oxford102/experiments/1543117119.2325218/vgg19_1543117119.2325218.h5\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 6s 714ms/step - loss: 24.5847 - acc: 0.0547 - val_loss: 24.4382 - val_acc: 0.0781\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.04688 to 0.07812, saving model to /data/oxford102/experiments/1543117119.2325218/vgg19_1543117119.2325218.h5\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 9s 1s/step - loss: 24.3585 - acc: 0.0753 - val_loss: 24.3634 - val_acc: 0.1094\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.07812 to 0.10938, saving model to /data/oxford102/experiments/1543117119.2325218/vgg19_1543117119.2325218.h5\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 6s 719ms/step - loss: 23.7793 - acc: 0.1289 - val_loss: 23.9409 - val_acc: 0.1250\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.10938 to 0.12500, saving model to /data/oxford102/experiments/1543117119.2325218/vgg19_1543117119.2325218.h5\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 6s 702ms/step - loss: 23.6774 - acc: 0.1641 - val_loss: 23.6131 - val_acc: 0.1719\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.12500 to 0.17188, saving model to /data/oxford102/experiments/1543117119.2325218/vgg19_1543117119.2325218.h5\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 6s 708ms/step - loss: 23.5631 - acc: 0.1797 - val_loss: 23.5049 - val_acc: 0.1719\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.17188\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 6s 705ms/step - loss: 23.3955 - acc: 0.1992 - val_loss: 23.0059 - val_acc: 0.2656\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.17188 to 0.26562, saving model to /data/oxford102/experiments/1543117119.2325218/vgg19_1543117119.2325218.h5\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 6s 714ms/step - loss: 23.2128 - acc: 0.2031 - val_loss: 23.1890 - val_acc: 0.2031\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.26562\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 6s 711ms/step - loss: 23.1702 - acc: 0.2266 - val_loss: 22.8874 - val_acc: 0.2500\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.26562\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 6s 768ms/step - loss: 22.9448 - acc: 0.2773 - val_loss: 23.0798 - val_acc: 0.2656\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.26562\n",
      "Epoch 00011: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 512, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543117119.2325218}\n"
     ]
    }
   ],
   "source": [
    "#run_training(model_name, _model, train_generator, validation_generator, params, num_train_img, num_val_img):\n",
    "    \n",
    "\n",
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 512,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 35s 1s/step - loss: 22.4890 - acc: 0.3179 - val_loss: 22.6014 - val_acc: 0.2930\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.29297, saving model to /data/oxford102/experiments/1543117217.7432935/vgg19_1543117217.7432935.h5\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 29s 834ms/step - loss: 22.1251 - acc: 0.3571 - val_loss: 22.2259 - val_acc: 0.2969\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.29297 to 0.29688, saving model to /data/oxford102/experiments/1543117217.7432935/vgg19_1543117217.7432935.h5\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 28s 796ms/step - loss: 21.8851 - acc: 0.3759 - val_loss: 21.8994 - val_acc: 0.3555\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.29688 to 0.35547, saving model to /data/oxford102/experiments/1543117217.7432935/vgg19_1543117217.7432935.h5\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 28s 791ms/step - loss: 21.6039 - acc: 0.4170 - val_loss: 21.5058 - val_acc: 0.4336\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.35547 to 0.43359, saving model to /data/oxford102/experiments/1543117217.7432935/vgg19_1543117217.7432935.h5\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 34s 971ms/step - loss: 21.2228 - acc: 0.4786 - val_loss: 21.2143 - val_acc: 0.4492\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.43359 to 0.44922, saving model to /data/oxford102/experiments/1543117217.7432935/vgg19_1543117217.7432935.h5\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 28s 812ms/step - loss: 20.8232 - acc: 0.5268 - val_loss: 21.1262 - val_acc: 0.4297\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.44922\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 28s 804ms/step - loss: 20.6786 - acc: 0.5045 - val_loss: 20.5654 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.44922 to 0.50000, saving model to /data/oxford102/experiments/1543117217.7432935/vgg19_1543117217.7432935.h5\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 28s 796ms/step - loss: 20.3351 - acc: 0.5509 - val_loss: 20.5188 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.50000\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 32s 906ms/step - loss: 20.1325 - acc: 0.5688 - val_loss: 20.4035 - val_acc: 0.4766\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.50000\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 28s 803ms/step - loss: 19.8450 - acc: 0.5824 - val_loss: 19.9795 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.50000\n",
      "Epoch 00010: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543117217.7432935}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 128,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 71s 1s/step - loss: 19.4662 - acc: 0.5977 - val_loss: 19.4825 - val_acc: 0.5423\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.54228, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 61s 859ms/step - loss: 19.0476 - acc: 0.6107 - val_loss: 18.9521 - val_acc: 0.5827\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.54228 to 0.58272, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 70s 980ms/step - loss: 18.5245 - acc: 0.6488 - val_loss: 18.4785 - val_acc: 0.6048\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.58272 to 0.60478, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 61s 866ms/step - loss: 18.0958 - acc: 0.6530 - val_loss: 18.0835 - val_acc: 0.6066\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.60478 to 0.60662, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 71s 1s/step - loss: 17.5373 - acc: 0.6922 - val_loss: 17.5167 - val_acc: 0.6158\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.60662 to 0.61581, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 62s 867ms/step - loss: 17.1074 - acc: 0.6796 - val_loss: 17.0520 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.61581 to 0.64522, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 68s 953ms/step - loss: 16.6295 - acc: 0.7116 - val_loss: 16.6719 - val_acc: 0.6360\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.64522\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 62s 872ms/step - loss: 16.2015 - acc: 0.7267 - val_loss: 16.2160 - val_acc: 0.6544\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.64522 to 0.65441, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 67s 943ms/step - loss: 15.7402 - acc: 0.7468 - val_loss: 15.8113 - val_acc: 0.6710\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.65441 to 0.67096, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 63s 881ms/step - loss: 15.3601 - acc: 0.7566 - val_loss: 15.3336 - val_acc: 0.6893\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.67096 to 0.68934, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 66s 934ms/step - loss: 14.9768 - acc: 0.7586 - val_loss: 15.0540 - val_acc: 0.6857\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.68934\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 62s 875ms/step - loss: 14.5780 - acc: 0.7768 - val_loss: 14.6380 - val_acc: 0.6985\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.68934 to 0.69853, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 13/100\n",
      "71/71 [==============================] - 66s 936ms/step - loss: 14.2287 - acc: 0.7711 - val_loss: 14.3313 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69853\n",
      "Epoch 14/100\n",
      "71/71 [==============================] - 62s 877ms/step - loss: 13.9440 - acc: 0.7657 - val_loss: 13.9967 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69853\n",
      "Epoch 15/100\n",
      "71/71 [==============================] - 66s 934ms/step - loss: 13.5421 - acc: 0.7927 - val_loss: 13.6480 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.69853 to 0.71875, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - 62s 875ms/step - loss: 13.1956 - acc: 0.7962 - val_loss: 13.3637 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.71875\n",
      "Epoch 17/100\n",
      "71/71 [==============================] - 67s 946ms/step - loss: 12.8858 - acc: 0.7926 - val_loss: 13.0398 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.71875 to 0.72243, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - 62s 874ms/step - loss: 12.5629 - acc: 0.7896 - val_loss: 12.7046 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.72243\n",
      "Epoch 19/100\n",
      "71/71 [==============================] - 64s 900ms/step - loss: 12.2906 - acc: 0.7870 - val_loss: 12.4055 - val_acc: 0.7279\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.72243 to 0.72794, saving model to /data/oxford102/experiments/1543117533.4054904/vgg19_1543117533.4054904.h5\n",
      "Epoch 20/100\n",
      "71/71 [==============================] - 62s 872ms/step - loss: 11.9688 - acc: 0.8046 - val_loss: 12.1675 - val_acc: 0.7096\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.72794\n",
      "Epoch 21/100\n",
      "71/71 [==============================] - 63s 893ms/step - loss: 11.7043 - acc: 0.8019 - val_loss: 11.9336 - val_acc: 0.6857\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.72794\n",
      "Epoch 22/100\n",
      "71/71 [==============================] - 67s 938ms/step - loss: 11.4546 - acc: 0.7990 - val_loss: 11.6542 - val_acc: 0.7040\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.72794\n",
      "Epoch 00022: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 64, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543117533.4054904}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 64,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "143/143 [==============================] - 143s 998ms/step - loss: 14.5837 - acc: 0.0446 - val_loss: 14.1087 - val_acc: 0.0533\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.05331, saving model to /data/oxford102/experiments/1543119372.975714/vgg19_1543119372.975714.h5\n",
      "Epoch 2/100\n",
      "  2/143 [..............................] - ETA: 1:48 - loss: 13.9183 - acc: 0.0625"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 32,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNE TOP LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers[11:]):\n",
    "    print(i, layer.name)\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 64,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
