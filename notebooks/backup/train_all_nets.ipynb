{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.applications.mobilenet_v2 import decode_predictions\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow\n",
    "from time import time\n",
    "\n",
    "import pandas\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train all nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15504574288862770091\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11280557671\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13008700569799606219\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MobileNetV2',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'absolute_import',\n",
       " 'decode_predictions',\n",
       " 'division',\n",
       " 'keras_modules_injection',\n",
       " 'mobilenet_v2',\n",
       " 'preprocess_input',\n",
       " 'print_function']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(keras.applications.mobilenet_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/oxford102/train'\n",
    "train_data_dir = \"/data/oxford102/train/\"\n",
    "validation_data_dir = \"/data/oxford102/train/\"\n",
    "img_width, img_height = 256, 256\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "nr_categories = 102\n",
    "nb_train_samples = 4604\n",
    "nb_validation_samples = 1094 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_generator(input_processor, img_aug=False):\n",
    "    if not img_aug:\n",
    "        train_val_datagen = ImageDataGenerator(preprocessing_function=input_processor, \n",
    "                                           validation_split=0.2)\n",
    "    else: \n",
    "        train_val_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            preprocessing_function=input_processor,\n",
    "            validation_split=0.2)\n",
    "        \n",
    "    return train_val_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generators(batch_size, image_size, input_processor, img_aug=False):\n",
    "\n",
    "    img_width, img_height = image_size\n",
    "    \n",
    "    train_val_datagen = get_image_generator(input_processor, img_aug)\n",
    "\n",
    "    train_generator = train_val_datagen.flow_from_directory(\n",
    "            train_data_dir,  # this is the target directory\n",
    "            target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "            batch_size=batch_size,\n",
    "            subset=\"training\",\n",
    "            class_mode='categorical')\n",
    "\n",
    "    validation_generator = train_val_datagen.flow_from_directory(\n",
    "            train_data_dir,  # this is the target directory\n",
    "            target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "            subset=\"validation\",\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical')\n",
    "    \n",
    "    return train_generator, validation_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import regularizers\n",
    "\n",
    "def get_model(network_name=\"inception_resnet_v2\", image_size=(256, 256), verbose=False):\n",
    "    k.set_learning_phase(0)\n",
    "\n",
    "    img_width, img_height = image_size\n",
    "    if network_name == \"vgg16\":\n",
    "        base_model = keras.applications.vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "        input_processor = applications.vgg16.preprocess_input\n",
    "    elif network_name == \"vgg19\":\n",
    "        base_model = keras.applications.vgg19.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "        input_processor = applications.vgg19.preprocess_input\n",
    "    elif network_name == \"inception_resnet_v2\":\n",
    "        base_model = keras.applications.inception_resnet_v2.InceptionResNetV2(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "        input_processor = applications.inception_resnet_v2.preprocess_input\n",
    "    elif network_name == \"mobilenet_v2\":\n",
    "        base_model = keras.applications.mobilenet_v2.MobileNetV2(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "        input_processor = applications.mobilenet_v2.preprocess_input\n",
    "    elif network_name == \"xception\":\n",
    "        base_model = keras.applications.xception.Xception(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "        input_processor = applications.xception.preprocess_input\n",
    "    elif network_name == \"resnet50\":\n",
    "        base_model = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "        input_processor = applications.resnet50.preprocess_input    \n",
    "    elif network_name == \"inception_v3\":\n",
    "        base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "        input_processor = applications.inception_v3.preprocess_input\n",
    "    elif network_name == \"mobilenet\":\n",
    "        base_model = keras.applications.mobilenet.MobileNet(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "        input_processor = applications.mobilenet.preprocess_input\n",
    "    elif network_name == \"densenet\":\n",
    "        base_model = keras.applications.densenet.DenseNet201(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "        input_processor = applications.densenet.preprocess_input        \n",
    "    elif network_name == \"nasnet\":\n",
    "        base_model = keras.applications.nasnet.NASNetMobile(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "        input_processor = applications.nasnet.preprocess_input\n",
    "    else:\n",
    "        raise Exception(\"check your network name\")\n",
    "\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "        #Adding custom Layers \n",
    "    k.set_learning_phase(1)\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\", \n",
    "              #kernel_regularizer=regularizers.l2(0.01),\n",
    "             #       activity_regularizer=regularizers.l1(0.001)\n",
    "             )(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x, training=True)\n",
    "    #x = Dense(102, activation=\"relu\")(x)\n",
    "    predictions = Dense(nr_categories, activation=\"softmax\")(x)\n",
    "\n",
    "    _model = Model(input = base_model.input, output = predictions)\n",
    "    if verbose:\n",
    "        _model.summary()\n",
    "    return _model, input_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params, _model, generators):\n",
    "    model_name = params[\"network_name\"]\n",
    "    num_train_img = 4604\n",
    "    num_val_img = 1094\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    log_time = time()\n",
    "    params['log_time'] = log_time\n",
    "    batch_size = params.get(\"batch_size\")\n",
    "\n",
    "    train_generator, validation_generator = generators\n",
    "    _model.compile(loss = \"categorical_crossentropy\", optimizer = params[\"optimizer\"], metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    base = '/data/oxford102/experiments'\n",
    "    path = os.path.join(base, str(log_time))\n",
    "    checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "    csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        history_callback = _model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=num_train_img // params[\"batch_size\"] // 2,\n",
    "                epochs=params[\"epochs\"],\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=num_val_img // params[\"batch_size\"],\n",
    "                callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "    finally:\n",
    "        params.pop(\"optimizer\")\n",
    "        pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "        _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "        print(params)\n",
    "        params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd =  optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamamx = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [\"mobilenet_v2\", \"densenet\", \"nasnet\",  \"vgg16\", \"vgg19\", \"inception_resnet_v2\", \"mobilenet\", \"resnet50\", \"inception_v3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'network_name': None,\n",
    "         'image_aug': False,\n",
    "         'optimizer_name': False, \n",
    "         'optimizer': None,\n",
    "         'optimizer_params': None, \n",
    "         'batch_size': 128,\n",
    "         'epochs': 100,\n",
    "         'image_size': (224, 224),\n",
    "         'log_time': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all nets with rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net: mobilenet_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 18s 1s/step - loss: 2.4331 - acc: 0.4752 - val_loss: 1.4218 - val_acc: 0.6689\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.66895, saving model to /data/oxford102/experiments/1546837399.7954097/mobilenet_v2_1546837399.7954097.h5\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 9s 556ms/step - loss: 1.1633 - acc: 0.7353 - val_loss: 0.9945 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.66895 to 0.77847, saving model to /data/oxford102/experiments/1546837399.7954097/mobilenet_v2_1546837399.7954097.h5\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 13s 764ms/step - loss: 0.3068 - acc: 0.9528 - val_loss: 0.7644 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77847 to 0.82298, saving model to /data/oxford102/experiments/1546837399.7954097/mobilenet_v2_1546837399.7954097.h5\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 14s 799ms/step - loss: 0.2202 - acc: 0.9683 - val_loss: 0.8753 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82298\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 13s 782ms/step - loss: 0.1241 - acc: 0.9852 - val_loss: 0.7021 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82298\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 14s 819ms/step - loss: 0.0768 - acc: 0.9945 - val_loss: 0.6932 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.82298 to 0.82505, saving model to /data/oxford102/experiments/1546837399.7954097/mobilenet_v2_1546837399.7954097.h5\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 14s 805ms/step - loss: 0.0537 - acc: 0.9977 - val_loss: 0.6216 - val_acc: 0.8395\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.82505 to 0.83954, saving model to /data/oxford102/experiments/1546837399.7954097/mobilenet_v2_1546837399.7954097.h5\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 14s 813ms/step - loss: 0.0405 - acc: 0.9968 - val_loss: 0.6609 - val_acc: 0.8354\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83954\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 14s 816ms/step - loss: 0.0321 - acc: 0.9982 - val_loss: 0.5874 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.83954 to 0.85404, saving model to /data/oxford102/experiments/1546837399.7954097/mobilenet_v2_1546837399.7954097.h5\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 14s 819ms/step - loss: 0.0168 - acc: 0.9991 - val_loss: 0.5697 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85404\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 14s 819ms/step - loss: 0.0118 - acc: 0.9995 - val_loss: 0.6040 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.85404\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 14s 838ms/step - loss: 0.0254 - acc: 0.9977 - val_loss: 0.6552 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85404\n",
      "Epoch 00012: early stopping\n",
      "{'network_name': 'mobilenet_v2', 'image_aug': False, 'optimizer_name': False, 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (224, 224), 'log_time': 1546837399.7954097}\n",
      "net: densenet\n",
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-618ec3d59b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"network_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_generators\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2d87d3214b81>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params, _model, generators)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'optimizer'"
     ]
    }
   ],
   "source": [
    "params['optimizer'] = rmsprop\n",
    "params['optimizer'] = \"rmsprop\"\n",
    "\n",
    "for net in nets:\n",
    "    print(\"net:\", net)\n",
    "    params['network_name'] = net\n",
    "    _model, input_processor = get_model(params[\"network_name\"], image_size=params[\"image_size\"])\n",
    "    train_generator, validation_generator = get_generators( params[\"batch_size\"], params[\"image_size\"], input_processor)\n",
    "    train_model(params, _model, (train_generator, validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net: densenet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.6281 - acc: 0.4554 - val_loss: 1.6247 - val_acc: 0.6768\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67676, saving model to /data/oxford102/experiments/1546838489.8699436/densenet_1546838489.8699436.h5\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 37s 2s/step - loss: 1.2682 - acc: 0.7619 - val_loss: 1.1237 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67676 to 0.79400, saving model to /data/oxford102/experiments/1546838489.8699436/densenet_1546838489.8699436.h5\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 40s 2s/step - loss: 0.4925 - acc: 0.9413 - val_loss: 0.7553 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.79400 to 0.84990, saving model to /data/oxford102/experiments/1546838489.8699436/densenet_1546838489.8699436.h5\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.3972 - acc: 0.9513 - val_loss: 0.7444 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84990 to 0.85818, saving model to /data/oxford102/experiments/1546838489.8699436/densenet_1546838489.8699436.h5\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.1696 - acc: 0.9884 - val_loss: 0.5355 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85818 to 0.89027, saving model to /data/oxford102/experiments/1546838489.8699436/densenet_1546838489.8699436.h5\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.1212 - acc: 0.9949 - val_loss: 0.6251 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.89027\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.0925 - acc: 0.9958 - val_loss: 0.5232 - val_acc: 0.8892\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.89027\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.0485 - acc: 0.9991 - val_loss: 0.4878 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89027\n",
      "Epoch 00008: early stopping\n",
      "{'network_name': 'densenet', 'image_aug': False, 'optimizer_name': False, 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (224, 224), 'log_time': 1546838489.8699436}\n",
      "net: nasnet\n",
      "Downloading data from https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile-no-top.h5\n",
      "19996672/19993432 [==============================] - 0s 0us/step\n",
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e074fc824801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"network_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_generators\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2d87d3214b81>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params, _model, generators)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'optimizer'"
     ]
    }
   ],
   "source": [
    "params['optimizer'] = rmsprop\n",
    "params['optimizer'] = \"rmsprop\"\n",
    "nets = [\"densenet\", \"nasnet\",  \"vgg16\", \"vgg19\", \"inception_resnet_v2\", \"mobilenet\", \"resnet50\", \"inception_v3\"]\n",
    "for net in nets:\n",
    "    print(\"net:\", net)\n",
    "    params['network_name'] = net\n",
    "    _model, input_processor = get_model(params[\"network_name\"], image_size=params[\"image_size\"])\n",
    "    train_generator, validation_generator = get_generators( params[\"batch_size\"], params[\"image_size\"], input_processor)\n",
    "    train_model(params, _model, (train_generator, validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net: nasnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 24s 1s/step - loss: 3.3142 - acc: 0.2973 - val_loss: 2.4934 - val_acc: 0.4189\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.41895, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 14s 840ms/step - loss: 2.2334 - acc: 0.4821 - val_loss: 1.9245 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.41895 to 0.53727, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 14s 844ms/step - loss: 1.3298 - acc: 0.7153 - val_loss: 1.7389 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.53727 to 0.57246, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 14s 820ms/step - loss: 1.2094 - acc: 0.7330 - val_loss: 1.6107 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57246 to 0.59938, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 14s 813ms/step - loss: 0.7822 - acc: 0.8507 - val_loss: 1.3700 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.59938 to 0.66046, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 14s 823ms/step - loss: 0.6877 - acc: 0.8801 - val_loss: 1.3401 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.66046 to 0.67702, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 14s 814ms/step - loss: 0.5107 - acc: 0.9151 - val_loss: 1.2630 - val_acc: 0.6822\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.67702 to 0.68219, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 14s 820ms/step - loss: 0.4200 - acc: 0.9375 - val_loss: 1.1744 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.68219 to 0.69151, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 14s 827ms/step - loss: 0.3194 - acc: 0.9521 - val_loss: 1.1491 - val_acc: 0.7122\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.69151 to 0.71222, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 14s 832ms/step - loss: 0.2755 - acc: 0.9646 - val_loss: 1.0996 - val_acc: 0.7139\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.71222 to 0.71387, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 14s 822ms/step - loss: 0.2258 - acc: 0.9691 - val_loss: 1.0678 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.71387 to 0.72981, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 14s 826ms/step - loss: 0.1436 - acc: 0.9839 - val_loss: 1.1481 - val_acc: 0.7081\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.72981\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 14s 823ms/step - loss: 0.1616 - acc: 0.9802 - val_loss: 0.9617 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.72981 to 0.74845, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 14s 823ms/step - loss: 0.1032 - acc: 0.9885 - val_loss: 1.1557 - val_acc: 0.6977\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.74845\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 14s 820ms/step - loss: 0.1155 - acc: 0.9867 - val_loss: 0.9580 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.74845 to 0.75673, saving model to /data/oxford102/experiments/1546839689.4144263/nasnet_1546839689.4144263.h5\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 14s 826ms/step - loss: 0.0670 - acc: 0.9940 - val_loss: 1.0576 - val_acc: 0.7391\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.75673\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 14s 828ms/step - loss: 0.1070 - acc: 0.9866 - val_loss: 1.0807 - val_acc: 0.7081\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.75673\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 14s 822ms/step - loss: 0.0628 - acc: 0.9936 - val_loss: 0.9485 - val_acc: 0.7505\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.75673\n",
      "Epoch 00018: early stopping\n",
      "{'network_name': 'nasnet', 'image_aug': False, 'optimizer_name': False, 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (224, 224), 'log_time': 1546839689.4144263}\n",
      "net: vgg16\n",
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 41s 2s/step - loss: 2.4536 - acc: 0.4738 - val_loss: 1.4754 - val_acc: 0.6670\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.66699, saving model to /data/oxford102/experiments/1546840454.0631776/vgg16_1546840454.0631776.h5\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 1.1663 - acc: 0.7353 - val_loss: 1.0139 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.66699 to 0.76294, saving model to /data/oxford102/experiments/1546840454.0631776/vgg16_1546840454.0631776.h5\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 37s 2s/step - loss: 0.2674 - acc: 0.9579 - val_loss: 0.8677 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76294 to 0.79814, saving model to /data/oxford102/experiments/1546840454.0631776/vgg16_1546840454.0631776.h5\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.1481 - acc: 0.9835 - val_loss: 0.8304 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79814\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.0622 - acc: 0.9967 - val_loss: 0.6848 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79814 to 0.82402, saving model to /data/oxford102/experiments/1546840454.0631776/vgg16_1546840454.0631776.h5\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.0336 - acc: 0.9982 - val_loss: 0.8384 - val_acc: 0.8043\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82402\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.0196 - acc: 0.9995 - val_loss: 0.7447 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82402\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.0172 - acc: 0.9986 - val_loss: 0.7106 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.82402 to 0.82609, saving model to /data/oxford102/experiments/1546840454.0631776/vgg16_1546840454.0631776.h5\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.0115 - acc: 0.9995 - val_loss: 0.7530 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82609\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 29s 2s/step - loss: 0.0070 - acc: 0.9991 - val_loss: 0.7851 - val_acc: 0.8076\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82609\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 28s 2s/step - loss: 0.0135 - acc: 0.9991 - val_loss: 0.7130 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.82609\n",
      "Epoch 00011: early stopping\n",
      "{'network_name': 'vgg16', 'image_aug': False, 'optimizer_name': False, 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (224, 224), 'log_time': 1546840454.0631776}\n",
      "net: vgg19\n",
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 37s 2s/step - loss: 2.4561 - acc: 0.4793 - val_loss: 1.4881 - val_acc: 0.6787\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67871, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 1.2287 - acc: 0.7178 - val_loss: 1.0470 - val_acc: 0.7578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_acc improved from 0.67871 to 0.75776, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.3006 - acc: 0.9528 - val_loss: 0.9283 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75776 to 0.78364, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.1746 - acc: 0.9821 - val_loss: 0.8613 - val_acc: 0.7899\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78364 to 0.78986, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.0731 - acc: 0.9968 - val_loss: 0.8051 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78986 to 0.80331, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.0482 - acc: 0.9963 - val_loss: 0.7716 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.80331 to 0.80849, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.0252 - acc: 0.9995 - val_loss: 0.7496 - val_acc: 0.8137\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.80849 to 0.81366, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.0160 - acc: 0.9995 - val_loss: 0.7802 - val_acc: 0.8023\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81366\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.0150 - acc: 0.9991 - val_loss: 0.7219 - val_acc: 0.8126\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81366\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.0093 - acc: 0.9995 - val_loss: 0.7973 - val_acc: 0.8174\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.81366 to 0.81738, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.0129 - acc: 0.9991 - val_loss: 0.8131 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.81738\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.7212 - val_acc: 0.8199\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.81738 to 0.81988, saving model to /data/oxford102/experiments/1546840864.926986/vgg19_1546840864.926986.h5\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.0048 - acc: 0.9991 - val_loss: 0.7824 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81988\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 34s 2s/step - loss: 0.0041 - acc: 0.9995 - val_loss: 0.8008 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81988\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 33s 2s/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.7016 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81988\n",
      "Epoch 00015: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer_name': False, 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (224, 224), 'log_time': 1546840864.926986}\n",
      "net: inception_resnet_v2\n",
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 48s 3s/step - loss: 3.3168 - acc: 0.2739 - val_loss: 2.5107 - val_acc: 0.4258\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.42578, saving model to /data/oxford102/experiments/1546841525.9154773/inception_resnet_v2_1546841525.9154773.h5\n"
     ]
    }
   ],
   "source": [
    "nets = [\"nasnet\",  \"vgg16\", \"vgg19\", \"inception_resnet_v2\", \"mobilenet\", \"resnet50\", \"inception_v3\"]\n",
    "for net in nets:\n",
    "    params['optimizer'] = rmsprop\n",
    "    params['optimizer'] = \"rmsprop\"\n",
    "    print(\"net:\", net)\n",
    "    params['network_name'] = net\n",
    "    _model, input_processor = get_model(params[\"network_name\"], image_size=params[\"image_size\"])\n",
    "    train_generator, validation_generator = get_generators( params[\"batch_size\"], params[\"image_size\"], input_processor)\n",
    "    train_model(params, _model, (train_generator, validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [\"inception_resnet_v2\", \"mobilenet\", \"resnet50\", \"inception_v3\"]\n",
    "for net in nets:\n",
    "    params['optimizer'] = rmsprop\n",
    "    params['optimizer'] = \"rmsprop\"\n",
    "    print(\"net:\", net)\n",
    "    params['network_name'] = net\n",
    "    _model, input_processor = get_model(params[\"network_name\"], image_size=params[\"image_size\"])\n",
    "    train_generator, validation_generator = get_generators( params[\"batch_size\"], params[\"image_size\"], input_processor)\n",
    "    train_model(params, _model, (train_generator, validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
