{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow\n",
    "from time import time\n",
    "\n",
    "import pandas\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/oxford102/train'\n",
    "img_width, img_height = 256, 256\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "nr_categories = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"/data/oxford102/train/\"\n",
    "validation_data_dir = \"/data/oxford102/train/\"\n",
    "nb_train_samples = 4604\n",
    "nb_validation_samples = 1094 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processor = keras.applications.vgg19.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_datagen = ImageDataGenerator(preprocessing_function=input_processor, \n",
    "                                           validation_split=0.2)\n",
    "\n",
    "train_val_datagen_aug = ImageDataGenerator(\n",
    "        #rescale=1. / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=input_processor,\n",
    "        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "train_generator = train_val_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        batch_size=batch_size,\n",
    "        subset=\"training\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_val_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        subset=\"validation\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "#from keras import regularizers\n",
    "\n",
    "k.set_learning_phase(0)\n",
    "\n",
    "network_name = \"vgg19\"\n",
    "img_width, img_height = (256, 256)\n",
    "if network_name == \"vgg16\":\n",
    "    base_model = keras.applications.vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "elif network_name == \"vgg19\":\n",
    "    base_model = keras.applications.vgg19.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "else:\n",
    "    raise Exception(\"check your network name\")\n",
    "\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    #Adding custom Layers \n",
    "k.set_learning_phase(1)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\", \n",
    "          #kernel_regularizer=regularizers.l2(0.01),\n",
    "         #       activity_regularizer=regularizers.l1(0.001)\n",
    "         )(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "#x = Dense(102, activation=\"relu\")(x)\n",
    "predictions = Dense(nr_categories, activation=\"softmax\")(x)\n",
    "\n",
    "_model = Model(input = base_model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 102)               104550    \n",
      "=================================================================\n",
      "Total params: 53,688,486\n",
      "Trainable params: 33,662,054\n",
      "Non-trainable params: 20,026,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 17s 1s/step - loss: 5.0826 - acc: 0.0239 - val_loss: 4.3120 - val_acc: 0.0859\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.08594, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 10s 565ms/step - loss: 4.0067 - acc: 0.1618 - val_loss: 3.3736 - val_acc: 0.2891\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.08594 to 0.28906, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 10s 570ms/step - loss: 3.2117 - acc: 0.2868 - val_loss: 3.1293 - val_acc: 0.2891\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.28906\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 10s 569ms/step - loss: 2.8625 - acc: 0.3640 - val_loss: 2.6578 - val_acc: 0.4297\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.28906 to 0.42969, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 10s 569ms/step - loss: 2.5071 - acc: 0.4320 - val_loss: 2.4481 - val_acc: 0.4453\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.42969 to 0.44531, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 10s 583ms/step - loss: 2.3848 - acc: 0.4559 - val_loss: 2.2323 - val_acc: 0.4844\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.44531 to 0.48438, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 10s 574ms/step - loss: 2.2930 - acc: 0.4890 - val_loss: 2.0409 - val_acc: 0.5156\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.48438 to 0.51562, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 10s 568ms/step - loss: 1.9810 - acc: 0.5717 - val_loss: 1.8689 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.51562 to 0.54688, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 10s 587ms/step - loss: 1.5967 - acc: 0.6654 - val_loss: 1.8118 - val_acc: 0.6484\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.54688 to 0.64844, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 10s 578ms/step - loss: 1.0961 - acc: 0.8015 - val_loss: 1.6200 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.64844 to 0.65625, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 10s 574ms/step - loss: 1.0433 - acc: 0.7960 - val_loss: 1.5142 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.65625 to 0.67969, saving model to /data/oxford102/experiments/1543113644.933369/vgg19_1543113644.933369.h5\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 10s 570ms/step - loss: 0.9248 - acc: 0.8493 - val_loss: 1.5368 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.67969\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 10s 571ms/step - loss: 1.0799 - acc: 0.7933 - val_loss: 1.5576 - val_acc: 0.6719\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.67969\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 10s 573ms/step - loss: 1.0042 - acc: 0.8015 - val_loss: 1.5049 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.67969\n",
      "Epoch 00014: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 256, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543113644.933369}\n"
     ]
    }
   ],
   "source": [
    "#run_training(model_name, _model, train_generator, validation_generator, params, num_train_img, num_val_img):\n",
    "    \n",
    "\n",
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 256,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 21s 591ms/step - loss: 0.6700 - acc: 0.9134 - val_loss: 1.4278 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67969, saving model to /data/oxford102/experiments/1543113850.0769787/vgg19_1543113850.0769787.h5\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.7015 - acc: 0.8830 - val_loss: 1.4974 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.67969\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.7379 - acc: 0.8777 - val_loss: 1.3917 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67969 to 0.70312, saving model to /data/oxford102/experiments/1543113850.0769787/vgg19_1543113850.0769787.h5\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 20s 557ms/step - loss: 0.6602 - acc: 0.8936 - val_loss: 1.3292 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.70312 to 0.70703, saving model to /data/oxford102/experiments/1543113850.0769787/vgg19_1543113850.0769787.h5\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 21s 588ms/step - loss: 0.4931 - acc: 0.9366 - val_loss: 1.2624 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70703 to 0.73047, saving model to /data/oxford102/experiments/1543113850.0769787/vgg19_1543113850.0769787.h5\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.4308 - acc: 0.9491 - val_loss: 1.2464 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.73047\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.3941 - acc: 0.9589 - val_loss: 1.2751 - val_acc: 0.7109\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.73047\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.4113 - acc: 0.9527 - val_loss: 1.2432 - val_acc: 0.7227\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.73047\n",
      "Epoch 00008: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543113850.0769787}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 128,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 40s 568ms/step - loss: 0.2747 - acc: 0.9798 - val_loss: 1.1395 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75368, saving model to /data/oxford102/experiments/1543114085.991548/vgg19_1543114085.991548.h5\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 40s 557ms/step - loss: 0.2769 - acc: 0.9801 - val_loss: 1.1201 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75368 to 0.76287, saving model to /data/oxford102/experiments/1543114085.991548/vgg19_1543114085.991548.h5\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 40s 560ms/step - loss: 0.1837 - acc: 0.9921 - val_loss: 1.0271 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76287 to 0.77022, saving model to /data/oxford102/experiments/1543114085.991548/vgg19_1543114085.991548.h5\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 40s 560ms/step - loss: 0.1932 - acc: 0.9885 - val_loss: 1.1312 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77022\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 40s 562ms/step - loss: 0.1405 - acc: 0.9952 - val_loss: 1.0699 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77022\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 40s 560ms/step - loss: 0.1472 - acc: 0.9938 - val_loss: 1.0185 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77022 to 0.77390, saving model to /data/oxford102/experiments/1543114085.991548/vgg19_1543114085.991548.h5\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 40s 561ms/step - loss: 0.1136 - acc: 0.9960 - val_loss: 1.0129 - val_acc: 0.7721\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77390\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 40s 560ms/step - loss: 0.1113 - acc: 0.9974 - val_loss: 0.9871 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.77390 to 0.79596, saving model to /data/oxford102/experiments/1543114085.991548/vgg19_1543114085.991548.h5\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 40s 559ms/step - loss: 0.0948 - acc: 0.9974 - val_loss: 0.9786 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79596\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 40s 558ms/step - loss: 0.0837 - acc: 0.9987 - val_loss: 0.9841 - val_acc: 0.7849\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79596\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 40s 561ms/step - loss: 0.0738 - acc: 0.9982 - val_loss: 0.9626 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79596\n",
      "Epoch 00011: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 64, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543114085.991548}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 64,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "143/143 [==============================] - 80s 559ms/step - loss: 0.0670 - acc: 0.9993 - val_loss: 1.0375 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75368, saving model to /data/oxford102/experiments/1543114611.679193/vgg19_1543114611.679193.h5\n",
      "Epoch 2/100\n",
      "143/143 [==============================] - 80s 557ms/step - loss: 0.0575 - acc: 0.9993 - val_loss: 0.9994 - val_acc: 0.7711\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75368 to 0.77114, saving model to /data/oxford102/experiments/1543114611.679193/vgg19_1543114611.679193.h5\n",
      "Epoch 3/100\n",
      "143/143 [==============================] - 80s 557ms/step - loss: 0.0521 - acc: 0.9998 - val_loss: 1.0095 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.77114\n",
      "Epoch 4/100\n",
      "143/143 [==============================] - 80s 557ms/step - loss: 0.0441 - acc: 0.9998 - val_loss: 0.9883 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77114 to 0.77849, saving model to /data/oxford102/experiments/1543114611.679193/vgg19_1543114611.679193.h5\n",
      "Epoch 5/100\n",
      "143/143 [==============================] - 80s 559ms/step - loss: 0.0414 - acc: 0.9998 - val_loss: 0.9912 - val_acc: 0.7711\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77849\n",
      "Epoch 6/100\n",
      "143/143 [==============================] - 80s 558ms/step - loss: 0.0359 - acc: 1.0000 - val_loss: 1.0078 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77849\n",
      "Epoch 7/100\n",
      "143/143 [==============================] - 80s 557ms/step - loss: 0.0352 - acc: 0.9998 - val_loss: 0.9803 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77849\n",
      "Epoch 00007: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 32, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543114611.679193}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 32,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNE TOP LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers[11:]):\n",
    "    print(i, layer.name)\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 16,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
