{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow\n",
    "from time import time\n",
    "\n",
    "import pandas\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/oxford102/train'\n",
    "img_width, img_height = 256, 256\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "nr_categories = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"/data/oxford102/train/\"\n",
    "validation_data_dir = \"/data/oxford102/train/\"\n",
    "nb_train_samples = 4604\n",
    "nb_validation_samples = 1094 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processor = keras.applications.vgg19.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_datagen = ImageDataGenerator(preprocessing_function=input_processor, \n",
    "                                           validation_split=0.2)\n",
    "\n",
    "train_val_datagen_aug = ImageDataGenerator(\n",
    "        #rescale=1. / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=input_processor,\n",
    "        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4604 images belonging to 102 classes.\n",
      "Found 1094 images belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "train_generator = train_val_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        batch_size=batch_size,\n",
    "        subset=\"training\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_val_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 250x250\n",
    "        subset=\"validation\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "k.set_learning_phase(0)\n",
    "\n",
    "network_name = \"vgg19\"\n",
    "img_width, img_height = (256, 256)\n",
    "if network_name == \"vgg16\":\n",
    "    base_model = keras.applications.vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "elif network_name == \"vgg19\":\n",
    "    base_model = keras.applications.vgg19.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
    "else:\n",
    "    raise Exception(\"check your network name\")\n",
    "\n",
    "for layer in base_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    #Adding custom Layers \n",
    "k.set_learning_phase(1)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\", \n",
    "          kernel_regularizer=regularizers.l2(0.01),\n",
    "         #       activity_regularizer=regularizers.l1(0.001)\n",
    "         )(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x, training=True)\n",
    "#x = Dense(102, activation=\"relu\")(x)\n",
    "predictions = Dense(nr_categories, activation=\"softmax\")(x)\n",
    "\n",
    "_model = Model(input = base_model.input, output = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 102)               104550    \n",
      "=================================================================\n",
      "Total params: 53,688,486\n",
      "Trainable params: 33,662,054\n",
      "Non-trainable params: 20,026,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 15.6746 - acc: 0.1523 - val_loss: 11.7794 - val_acc: 0.2656\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.26562, saving model to /data/oxford102/experiments/1543123258.7904139/vgg19_1543123258.7904139.h5\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 5s 632ms/step - loss: 10.7718 - acc: 0.3203 - val_loss: 9.8102 - val_acc: 0.3594\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.26562 to 0.35938, saving model to /data/oxford102/experiments/1543123258.7904139/vgg19_1543123258.7904139.h5\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 5s 627ms/step - loss: 9.1828 - acc: 0.4375 - val_loss: 8.2818 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.35938 to 0.53125, saving model to /data/oxford102/experiments/1543123258.7904139/vgg19_1543123258.7904139.h5\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 8s 991ms/step - loss: 7.9815 - acc: 0.5220 - val_loss: 7.5211 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.53125 to 0.56250, saving model to /data/oxford102/experiments/1543123258.7904139/vgg19_1543123258.7904139.h5\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 5s 620ms/step - loss: 7.2419 - acc: 0.5117 - val_loss: 6.8657 - val_acc: 0.4844\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.56250\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 5s 624ms/step - loss: 6.6383 - acc: 0.5742 - val_loss: 6.4440 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.56250\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 5s 623ms/step - loss: 6.0981 - acc: 0.6016 - val_loss: 6.0713 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.56250 to 0.59375, saving model to /data/oxford102/experiments/1543123258.7904139/vgg19_1543123258.7904139.h5\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 5s 627ms/step - loss: 5.7263 - acc: 0.6328 - val_loss: 5.4678 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.59375 to 0.60938, saving model to /data/oxford102/experiments/1543123258.7904139/vgg19_1543123258.7904139.h5\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 5s 634ms/step - loss: 5.2271 - acc: 0.6797 - val_loss: 5.3380 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60938\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 5s 629ms/step - loss: 5.2740 - acc: 0.5977 - val_loss: 5.0865 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.60938 to 0.64062, saving model to /data/oxford102/experiments/1543123258.7904139/vgg19_1543123258.7904139.h5\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 5s 628ms/step - loss: 5.3623 - acc: 0.5859 - val_loss: 5.1836 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.64062\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 5s 629ms/step - loss: 5.1597 - acc: 0.5977 - val_loss: 5.0829 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.64062\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 5s 628ms/step - loss: 4.8943 - acc: 0.6719 - val_loss: 5.0869 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.64062\n",
      "Epoch 00013: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 512, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543123258.7904139}\n"
     ]
    }
   ],
   "source": [
    "#run_training(model_name, _model, train_generator, validation_generator, params, num_train_img, num_val_img):\n",
    "    \n",
    "\n",
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 512,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 24s 682ms/step - loss: 5.1068 - acc: 0.6536 - val_loss: 5.2727 - val_acc: 0.5859\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58594, saving model to /data/oxford102/experiments/1543123363.7121034/vgg19_1543123363.7121034.h5\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 20s 582ms/step - loss: 4.6574 - acc: 0.7071 - val_loss: 4.6667 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.58594 to 0.66406, saving model to /data/oxford102/experiments/1543123363.7121034/vgg19_1543123363.7121034.h5\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 20s 585ms/step - loss: 4.5420 - acc: 0.6969 - val_loss: 4.7365 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.66406\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 20s 583ms/step - loss: 4.4573 - acc: 0.7045 - val_loss: 4.6627 - val_acc: 0.6719\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.66406 to 0.67188, saving model to /data/oxford102/experiments/1543123363.7121034/vgg19_1543123363.7121034.h5\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 20s 585ms/step - loss: 3.7016 - acc: 0.8330 - val_loss: 4.1954 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.67188\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 20s 585ms/step - loss: 3.8663 - acc: 0.7784 - val_loss: 4.3440 - val_acc: 0.6602\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67188\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 20s 584ms/step - loss: 4.0656 - acc: 0.7473 - val_loss: 4.4003 - val_acc: 0.6953\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.67188 to 0.69531, saving model to /data/oxford102/experiments/1543123363.7121034/vgg19_1543123363.7121034.h5\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 20s 584ms/step - loss: 4.1993 - acc: 0.7330 - val_loss: 4.3903 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69531\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 20s 585ms/step - loss: 3.6474 - acc: 0.8286 - val_loss: 4.0092 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69531\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 20s 585ms/step - loss: 3.6801 - acc: 0.7795 - val_loss: 4.4154 - val_acc: 0.6562\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69531\n",
      "Epoch 00010: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 128, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543123363.7121034}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 128,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 45s 634ms/step - loss: 4.4337 - acc: 0.7148 - val_loss: 4.4943 - val_acc: 0.6507\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65074, saving model to /data/oxford102/experiments/1543123582.61247/vgg19_1543123582.61247.h5\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 41s 583ms/step - loss: 4.2790 - acc: 0.7271 - val_loss: 4.5790 - val_acc: 0.6581\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65074 to 0.65809, saving model to /data/oxford102/experiments/1543123582.61247/vgg19_1543123582.61247.h5\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 42s 585ms/step - loss: 3.6730 - acc: 0.8079 - val_loss: 4.1665 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.65809 to 0.66544, saving model to /data/oxford102/experiments/1543123582.61247/vgg19_1543123582.61247.h5\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 41s 583ms/step - loss: 3.9549 - acc: 0.7540 - val_loss: 4.3005 - val_acc: 0.6801\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.66544 to 0.68015, saving model to /data/oxford102/experiments/1543123582.61247/vgg19_1543123582.61247.h5\n",
      "Epoch 5/100\n",
      "71/71 [==============================] - 42s 585ms/step - loss: 3.5390 - acc: 0.8304 - val_loss: 4.2387 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.68015\n",
      "Epoch 6/100\n",
      "71/71 [==============================] - 42s 585ms/step - loss: 3.9538 - acc: 0.7614 - val_loss: 4.2264 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.68015 to 0.68199, saving model to /data/oxford102/experiments/1543123582.61247/vgg19_1543123582.61247.h5\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - 41s 584ms/step - loss: 3.5079 - acc: 0.8172 - val_loss: 4.1724 - val_acc: 0.6268\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.68199\n",
      "Epoch 8/100\n",
      "71/71 [==============================] - 41s 584ms/step - loss: 3.9555 - acc: 0.7452 - val_loss: 4.3457 - val_acc: 0.6801\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.68199\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - 41s 584ms/step - loss: 3.5659 - acc: 0.8085 - val_loss: 4.0408 - val_acc: 0.6893\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.68199 to 0.68934, saving model to /data/oxford102/experiments/1543123582.61247/vgg19_1543123582.61247.h5\n",
      "Epoch 10/100\n",
      "71/71 [==============================] - 41s 584ms/step - loss: 3.8544 - acc: 0.7698 - val_loss: 4.2479 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.68934\n",
      "Epoch 11/100\n",
      "71/71 [==============================] - 42s 585ms/step - loss: 3.5211 - acc: 0.8160 - val_loss: 4.0951 - val_acc: 0.6710\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.68934\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - 41s 584ms/step - loss: 3.9737 - acc: 0.7427 - val_loss: 4.3576 - val_acc: 0.6489\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.68934\n",
      "Epoch 00012: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 64, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543123582.61247}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 64,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "143/143 [==============================] - 86s 603ms/step - loss: 4.2991 - acc: 0.7248 - val_loss: 4.3350 - val_acc: 0.6737\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67371, saving model to /data/oxford102/experiments/1543124101.3124378/vgg19_1543124101.3124378.h5\n",
      "Epoch 2/100\n",
      "143/143 [==============================] - 83s 581ms/step - loss: 3.7353 - acc: 0.7893 - val_loss: 4.2369 - val_acc: 0.6838\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67371 to 0.68382, saving model to /data/oxford102/experiments/1543124101.3124378/vgg19_1543124101.3124378.h5\n",
      "Epoch 3/100\n",
      "143/143 [==============================] - 83s 583ms/step - loss: 3.7341 - acc: 0.7803 - val_loss: 4.2320 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.68382 to 0.68842, saving model to /data/oxford102/experiments/1543124101.3124378/vgg19_1543124101.3124378.h5\n",
      "Epoch 4/100\n",
      "143/143 [==============================] - 83s 583ms/step - loss: 3.7558 - acc: 0.7773 - val_loss: 4.3568 - val_acc: 0.6443\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.68842\n",
      "Epoch 5/100\n",
      "143/143 [==============================] - 83s 583ms/step - loss: 3.7547 - acc: 0.7891 - val_loss: 4.2572 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.68842\n",
      "Epoch 6/100\n",
      "143/143 [==============================] - 83s 582ms/step - loss: 3.6986 - acc: 0.7828 - val_loss: 4.2481 - val_acc: 0.6682\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.68842\n",
      "Epoch 00006: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 32, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543124101.3124378}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 32,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNE TOP LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 block3_pool\n",
      "1 block4_conv1\n",
      "2 block4_conv2\n",
      "3 block4_conv3\n",
      "4 block4_conv4\n",
      "5 block4_pool\n",
      "6 block5_conv1\n",
      "7 block5_conv2\n",
      "8 block5_conv3\n",
      "9 block5_conv4\n",
      "10 block5_pool\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers[11:]):\n",
    "    print(i, layer.name)\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "71/71 [==============================] - 61s 864ms/step - loss: 5.5933 - acc: 0.0541 - val_loss: 5.0694 - val_acc: 0.0625\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.06250, saving model to /data/oxford102/experiments/1543124611.9186873/vgg19_1543124611.9186873.h5\n",
      "Epoch 2/100\n",
      "71/71 [==============================] - 58s 812ms/step - loss: 5.1959 - acc: 0.0502 - val_loss: 5.0648 - val_acc: 0.0257\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.06250\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - 57s 796ms/step - loss: 5.2493 - acc: 0.0538 - val_loss: 5.2544 - val_acc: 0.0588\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.06250\n",
      "Epoch 4/100\n",
      "71/71 [==============================] - 57s 796ms/step - loss: 5.1794 - acc: 0.0515 - val_loss: 5.3930 - val_acc: 0.0460\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.06250\n",
      "Epoch 00004: early stopping\n",
      "{'network_name': 'vgg19', 'image_aug': False, 'optimizer': 'SGD', 'optimizer_params': None, 'batch_size': 64, 'epochs': 100, 'image_size': (256, 256), 'log_time': 1543124611.9186873}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vgg19\"\n",
    "num_train_img = 4604\n",
    "num_val_img = 1094\n",
    "params = {'network_name': 'vgg19',\n",
    "         'image_aug': False,\n",
    "         'optimizer': 'SGD',\n",
    "          'optimizer_params': None, \n",
    "         'batch_size': 64,\n",
    "         'epochs': 100,\n",
    "         'image_size': (256, 256),\n",
    "         'log_time': None}\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "log_time = time()\n",
    "params['log_time'] = log_time\n",
    "batch_size = params.get(\"batch_size\")\n",
    "\n",
    "_model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "base = '/data/oxford102/experiments'\n",
    "path = os.path.join(base, str(log_time))\n",
    "checkpoint = ModelCheckpoint(os.path.join(path, \"{}_{}.h5\".format(model_name, log_time)), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(log_time), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = CSVLogger(os.path.join(path, \"{}_{}.csv\".format(model_name, log_time)), append=True, separator=';')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    history_callback = _model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=num_train_img // batch_size,\n",
    "            epochs=params.get(\"epochs\"),\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=num_val_img // batch_size,\n",
    "            callbacks = [checkpoint, early, tensorboard, csv_logger])\n",
    "except Exception as e:\n",
    "    raise(e)\n",
    "finally:\n",
    "    pk.dump(params, open(\"experimental_params/experiments_{}.pk\".format(log_time), \"wb\"), protocol=pk.HIGHEST_PROTOCOL)\n",
    "    _model.save_weights(os.path.join(path, 'model_{}_weights_final_{}.h5'.format(model_name, log_time)))  # always save your weights after training or during training\n",
    "    print(params)\n",
    "    params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
